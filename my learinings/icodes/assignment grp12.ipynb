{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0249646-e12e-4df7-879e-70438d519e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # Changed train_selection to train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82807cc2-f02c-43d0-aa28-193022f401d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv( 'magicdataset.csv' )\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98841a4e-1647-4a2f-865c-011db3482651",
   "metadata": {},
   "outputs": [],
   "source": [
    "classG=df[df['class']== 'g' ]\n",
    "classH=df[df['class']== 'h' ]\n",
    "countG, countH = df['class'].value_counts()\n",
    "classGUnder = classG.sample(countH)\n",
    "newDataset = pd.concat([classGUnder, classH], axis=0)\n",
    "newDataset.to_csv('balanced_dataset.csv',index=False)\n",
    "# df1=pd.read_csv( 'balanced_dataset.csv' )\n",
    "# df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a851a-0c8c-4752-a974-085ca1df56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataset = pd.read_csv('balanced_dataset.csv')\n",
    "newDataset['class'].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c039b42-f85b-4812-a7f6-ccf2983ecfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = newDataset.drop('class', axis=1) # 1 for column, 0 for index\n",
    "y = newDataset['class'] # Remove the trailing comma to avoid creating a tuple\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923bcc7-232f-4b84-89d8-c23ac326640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be73f88-3269-4914-8f5d-1c752406c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a516e07-e3e9-4f85-8146-3da0f5c70d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773de80-0af2-4319-af38-ebe13892a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = {\n",
    "    'binary_crossentropy': 'binary_crossentropy',\n",
    "    'hinge': 'hinge',\n",
    "    'focal_loss' : 'binary_crossentropy' #Placeholder for focal loss\n",
    "    }\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        loss = alpha * tf.pow(1 - y_pred, gamma) * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232375d-b3b5-4efe-b4ff-713bcee9ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\"\n",
    "column_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',\n",
    "                'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Preprocess the data\n",
    "data['class'] = data['class'].map({'g': 1, 'h': 0})  # Convert class labels to binary\n",
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# Split the dataset into majority and minority classes\n",
    "classG = data[data['class'] == 1]\n",
    "classH = data[data['class'] == 0]\n",
    "\n",
    "# Balance the dataset by undersampling the majority class\n",
    "countH = len(classH)\n",
    "classGUnder = classG.sample(countH)\n",
    "newDataset = pd.concat([classGUnder, classH], axis=0)\n",
    "\n",
    "# Split the balanced dataset\n",
    "X = newDataset.drop('class', axis=1)\n",
    "y = newDataset['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model with different loss functions\n",
    "loss_functions = {\n",
    "    'binary_crossentropy': 'binary_crossentropy',\n",
    "    'hinge': 'hinge',\n",
    "    'focal_loss': focal_loss(gamma=2.0, alpha=0.25)  # Use the custom focal loss function\n",
    "}\n",
    "\n",
    "# Custom focal loss function\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        loss = alpha * tf.pow(1 - y_pred, gamma) * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Train and evaluate the model with different loss functions\n",
    "for loss_name, loss_function in loss_functions.items():\n",
    "    print(f\"Training with loss function: {loss_name}\")\n",
    "\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy with {loss_name}: {test_accuracy:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4aa6a-d701-4c6c-8a27-0d56c435193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\"\n",
    "column_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',\n",
    "                'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Preprocess the data\n",
    "data['class'] = data['class'].map({'g': 1, 'h': 0})  # Convert class labels to binary\n",
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# Split the dataset into majority and minority classes\n",
    "classG = data[data['class'] == 1]\n",
    "classH = data[data['class'] == 0]\n",
    "\n",
    "# Balance the dataset by undersampling the majority class\n",
    "countH = len(classH)\n",
    "classGUnder = classG.sample(countH)\n",
    "newDataset = pd.concat([classGUnder, classH], axis=0)\n",
    "\n",
    "# Split the balanced dataset\n",
    "X = newDataset.drop('class', axis=1)\n",
    "y = newDataset['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Custom focal loss function\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        loss = alpha * tf.pow(1 - y_pred, gamma) * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model with different loss functions\n",
    "loss_functions = {\n",
    "    'binary_crossentropy': 'binary_crossentropy',\n",
    "    'hinge': 'hinge',\n",
    "    'focal_loss': focal_loss(gamma=2.0, alpha=0.25)  # Use the custom focal loss function\n",
    "}\n",
    "\n",
    "# Train and evaluate the model with different loss functions\n",
    "for loss_name, loss_function in loss_functions.items():\n",
    "    print(f\"Training with loss function: {loss_name}\")\n",
    "\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy with {loss_name}: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    report = classification_report(y_test, y_pred, target_names=['Class H (0)', 'Class G (1)'], output_dict=True)\n",
    "\n",
    "    precision = report['Class G (1)']['precision']\n",
    "    recall = report['Class G (1)']['recall']\n",
    "    f1_score = report['Class G (1)']['f1-score']\n",
    "\n",
    "    print(f\"Precision for Class G (1): {precision:.4f}\")\n",
    "    print(f\"Recall for Class G (1): {recall:.4f}\")\n",
    "    print(f\"F1 Score for Class G (1): {f1_score:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739fb6d-1206-4e0d-ab94-2e7686c43765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test and y_pred are already defined from your previous code\n",
    "# y_pred contains the binary predictions from the model\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class H (0)', 'Class G (1)'], yticklabels=['Class H (0)', 'Class G (1)'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
